{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0311d809",
   "metadata": {},
   "source": [
    "# 3주차 과제\n",
    "## 201721489 정다솜\n",
    "\n",
    "## 첫번째 과제 - 강의내용 정리하기\n",
    "\n",
    "## 1. 텍스트 마이닝의 이해를 위한 기본 요구 지식\n",
    "* 자연어 처리\n",
    "* 통계학 & 선형대수\n",
    "* 머신러닝\n",
    "* 딥러닝\n",
    "\n",
    "## 2. 텍스트 마이닝 적용분야\n",
    "* Document classification\n",
    "* Document generation\n",
    "* Keyword extraction\n",
    "* Topic modeling\n",
    "\n",
    "## 3. 텍스트 마이닝 도구\n",
    "* NLTK - 가장 많이 알려진 NLP 라이브러리\n",
    "* Scikit Learn - 머신러닝 라이브러리\n",
    "* Gensim - Word2Vec으로 유명\n",
    "* keras - RNN, seq2seq 등 딥러닝 위주의 라이브러리 제공\n",
    "* Tokenize - 대상이 되는 문서/문장을 최소 단위로 쪼갬\n",
    "* Text normalization - 최소 단위를 표준화\n",
    "* POS-tagging - 최소 의미단위로 나누어진 대상에 대해 품사를 부착\n",
    "* Chunking - POS-tagging의 결과를 명사구, 형용사구, 분사구 등과 같은 말모듬으로 다시 합치는 과정\n",
    "* BOW, TFIDF - tokenized 결과를 이용하여 문서를 vector로 표현\n",
    "\n",
    "## 4. 텍스트 마이닝 방법\n",
    "* Naïve Bayes - 단어를 분석하여 더 많은 확률을 가진 쪽으로 분류\n",
    "* Logistic Regression - 분류를 위한 회귀분석 / 텍스트 마이닝에서의 문제\n",
    "* Ridge and Lasso Regression - 과적합을 방지 / 영향을 거의 미치지 않는 단어들을 제외\n",
    "* Sentiment Analysis - 감성분석으로 단어들을 분류\n",
    "\n",
    "## 5. 텍스트 마이닝에서 발생하는 문제와 해결방법\n",
    "* 각 데이터 간의 거리가 너무 멀게 위치 - 더 많은 데이터 수집\n",
    "* 단어 빈도의 불균형 - 빈도 높은 단어를 삭제, 다양한 함수를 이용해 해결\n",
    "* 단어가 쓰인 순서정보의 손실 - n그램과 딥러닝으로 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c8fd4",
   "metadata": {},
   "source": [
    "## 두번째 과제 - 주석달기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9183e0",
   "metadata": {},
   "source": [
    "## 1. urllib\n",
    "* 웹 사이트에 있는 데이터를 추출하기 위해 urllib 라이브러리 사용\n",
    "* url을 다루는 모듈을 모아놓은 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "169d3703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 읽어들이기\n",
    "import urllib.request\n",
    "\n",
    "# png파일을 test.png라는 이름의 파일로 저장\n",
    "url=\"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename=\"test.png\"\n",
    "\n",
    "# urlretrieve함수를 이용하여 url 처리하기\n",
    "urllib.request.urlretrieve(url, savename)\n",
    "print(\"저장되었습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2305d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다..\n"
     ]
    }
   ],
   "source": [
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"test1.png\"\n",
    "\n",
    "# request.urlopen함수를 사용하여 다운로드\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "#파일로 저장하기, wb는 쓰기와 바이너리모드\n",
    "with open(savename, mode=\"wb\") as f:\n",
    "    f.write(mem)\n",
    "    print(\"저장되었습니다..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3e8723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ip]\n",
      "API_URI=http://api.aoikujira.com/ip/get.php\n",
      "REMOTE_ADDR=183.99.230.186\n",
      "REMOTE_HOST=183.99.230.186\n",
      "REMOTE_PORT=41678\n",
      "HTTP_HOST=api.aoikujira.com\n",
      "HTTP_USER_AGENT=Python-urllib/3.8\n",
      "HTTP_ACCEPT_LANGUAGE=\n",
      "HTTP_ACCEPT_CHARSET=\n",
      "SERVER_PORT=80\n",
      "FORMAT=ini\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#데이터 읽어들이기\n",
    "url = \"http://api.aoikujira.com/ip/ini\"\n",
    "res = urllib.request.urlopen(url)\n",
    "data = res.read()\n",
    "\n",
    "#바이너리를 문자열로 변환하기\n",
    "text = data.decode(\"utf-8\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cde119",
   "metadata": {},
   "source": [
    "## 2. BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4627a7",
   "metadata": {},
   "source": [
    "### 2.1 기본사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "866562d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 설치\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 예제 html 만들기\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <h1>스크레이핑이란?</h1>\n",
    "  <p>웹 페이지를 분석하는 것</p>\n",
    "  <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb3bcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html 분석하기\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a98b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 부분 추출하기\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32e3beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "p  = 웹 페이지를 분석하는 것\n",
      "p  = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# 요소의 글자 출력하기\n",
    "print(f\"h1 = {h1.string}\")\n",
    "print(f\"p  = {p1.string}\")\n",
    "print(f\"p  = {p2.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749713b3",
   "metadata": {},
   "source": [
    "### 2.2 요소를 찾는 method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46dbe63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72a3eb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>스크레이핑이란?</h1>\n"
     ]
    }
   ],
   "source": [
    "# find 매서드로 원하는 부분 추출하기\n",
    "title = soup.find(\"h1\")\n",
    "body  = soup.find(\"p\")\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0101a126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title = 스크레이핑이란?\n",
      "#body = 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 부분 출력하기\n",
    "print(f\"#title = {title.string}\" )\n",
    "print(f\"#body = {body.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62775e25",
   "metadata": {},
   "source": [
    "### 2.3 복수 elements 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c744ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <ul>\n",
    "    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "    <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "  </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "812583cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"http://www.daum.net\">daum</a>] 2\n"
     ]
    }
   ],
   "source": [
    "# find_all 매서드로 추출하기\n",
    "links = soup.find_all(\"a\")\n",
    "print(links, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ead6457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "# 링크 목록 출력하기\n",
    "for a in links:\n",
    "    href = a.attrs['href'] # href의 속성에 있는 속성값을 추출\n",
    "    text = a.string \n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e9f25",
   "metadata": {},
   "source": [
    "## 3. Css selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dae1b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 사이트에서 데이터 가져오기\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "<div id=\"meigen\">\n",
    "  <h1>위키북스 도서</h1>\n",
    "  <ul class=\"items\">\n",
    "    <li>유니티 게임 이펙트 입문</li>\n",
    "    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "    <li>모던 웹사이트 디자인의 정석</li>\n",
    "  </ul>\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 \n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6fa5572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 위키북스 도서\n",
      "li = 유니티 게임 이펙트 입문\n",
      "li = 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li = 모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "# 필요한 부분을 Css쿼리로 추출하기\n",
    "\n",
    "# 타이틀 부분 추출하기 --- (※3)\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "print(f\"h1 = {h1}\")\n",
    "\n",
    "# 목록 부분 추출하기 --- (※4)\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\")\n",
    "for li in li_list:\n",
    "  print(f\"li = {li.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062acd6",
   "metadata": {},
   "source": [
    "## 4. 활용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bf3a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 패키지 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493e20a",
   "metadata": {},
   "source": [
    "### 4.1 네이버 금융 - 환율 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac16d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML가져오기\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = request.urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f5d6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4293ef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw = 1,178.00\n"
     ]
    }
   ],
   "source": [
    "# 원하는 데이터 추출하기\n",
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"usd/krw =\", price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d866b32",
   "metadata": {},
   "source": [
    "### 4.2 기상청 RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c159df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url= http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\n"
     ]
    }
   ],
   "source": [
    "# HTML 가져오기\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "#매개변수를 URL로 인코딩한다.\n",
    "values = {\n",
    "    'stnId':'109'\n",
    "}\n",
    "\n",
    "params=parse.urlencode(values)\n",
    "url += \"?\"+params # URL에 매개변수 추가\n",
    "print(\"url=\", url)\n",
    "\n",
    "res = request.urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38477551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42aa6051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울,경기도 육상중기예보\n",
      "○ (강수) 29일(수)은 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 아침최저기온은 13~19도로 오늘(25일, 아침최저기온 15~20도)과 비슷하거나 조금 낮겠고, <br />          낮최고기온은 23~28도로 오늘(25일, 낮최고기온 23~24도)보다 높겠습니다.<br />○ (해상) 서해중부해상의 물결은 1.0~2.0m로 일겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# 원하는 데이터 추출하기\n",
    "header = soup.find(\"header\")\n",
    "\n",
    "title = header.find(\"title\").text\n",
    "wf = header.find(\"wf\").text\n",
    "\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0860c1",
   "metadata": {},
   "source": [
    "### 4.3 윤동주 작가의 작품 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a94521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 하늘과 바람과 별과 시\n",
      "- 증보판\n",
      "- 서시\n",
      "- 자화상\n",
      "- 소년\n",
      "- 눈 오는 지도\n",
      "- 돌아와 보는 밤\n",
      "- 병원\n",
      "- 새로운 길\n",
      "- 간판 없는 거리\n",
      "- 태초의 아침\n",
      "- 또 태초의 아침\n",
      "- 새벽이 올 때까지\n",
      "- 무서운 시간\n",
      "- 십자가\n",
      "- 바람이 불어\n",
      "- 슬픈 족속\n",
      "- 눈감고 간다\n",
      "- 또 다른 고향\n",
      "- 길\n",
      "- 별 헤는 밤\n",
      "- 흰 그림자\n",
      "- 사랑스런 추억\n",
      "- 흐르는 거리\n",
      "- 쉽게 씌어진 시\n",
      "- 봄\n",
      "- 참회록\n",
      "- 간(肝)\n",
      "- 위로\n",
      "- 팔복\n",
      "- 못자는밤\n",
      "- 달같이\n",
      "- 고추밭\n",
      "- 아우의 인상화\n",
      "- 사랑의 전당\n",
      "- 이적\n",
      "- 비오는 밤\n",
      "- 산골물\n",
      "- 유언\n",
      "- 창\n",
      "- 바다\n",
      "- 비로봉\n",
      "- 산협의 오후\n",
      "- 명상\n",
      "- 소낙비\n",
      "- 한난계\n",
      "- 풍경\n",
      "- 달밤\n",
      "- 장\n",
      "- 밤\n",
      "- 황혼이 바다가 되어\n",
      "- 아침\n",
      "- 빨래\n",
      "- 꿈은 깨어지고\n",
      "- 산림\n",
      "- 이런날\n",
      "- 산상\n",
      "- 양지쪽\n",
      "- 닭\n",
      "- 가슴 1\n",
      "- 가슴 2\n",
      "- 비둘기\n",
      "- 황혼\n",
      "- 남쪽 하늘\n",
      "- 창공\n",
      "- 거리에서\n",
      "- 삶과 죽음\n",
      "- 초한대\n",
      "- 산울림\n",
      "- 해바라기 얼굴\n",
      "- 귀뚜라미와 나와\n",
      "- 애기의 새벽\n",
      "- 햇빛·바람\n",
      "- 반디불\n",
      "- 둘 다\n",
      "- 거짓부리\n",
      "- 눈\n",
      "- 참새\n",
      "- 버선본\n",
      "- 편지\n",
      "- 봄\n",
      "- 무얼 먹구 사나\n",
      "- 굴뚝\n",
      "- 햇비\n",
      "- 빗자루\n",
      "- 기왓장 내외\n",
      "- 오줌싸개 지도\n",
      "- 병아리\n",
      "- 조개껍질\n",
      "- 겨울\n",
      "- 트루게네프의 언덕\n",
      "- 달을 쏘다\n",
      "- 별똥 떨어진 데\n",
      "- 화원에 꽃이 핀다\n",
      "- 종시\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = request.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# #mw-content-text 바로 아래에 있는 \n",
    "# ul 태그 바로 아래에 있는\n",
    "# li 태그 아래에 있는\n",
    "# a 태그를 모두 선택합니다.\n",
    "a_list = soup.select(\"#mw-content-text   ul > li  a\")\n",
    "for a in a_list:\n",
    "    name = a.string\n",
    "    print(f\"- {name}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1409f3a",
   "metadata": {},
   "source": [
    "### 4.4 네이버 뉴스 헤드라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3eee077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                        정은경 \"향후 1~2주 확진자 급증할 듯..사적 모임 자제 요청\"\n",
      "                                    \n",
      "\n",
      "                                        '최대 승부처' 민주당 광주·전남 경선 결과 곧 발표\n",
      "                                    \n",
      "\n",
      "                                        \"노엘 불구속은 아빠찬스\"…대학생단체, '장제원 사퇴' 요구하며 시위\n",
      "                                    \n",
      "\n",
      "                                        성남시민 9명, ‘대장동 의혹’ 성남의뜰 상대 소송\n",
      "                                    \n",
      "\n",
      "                                        윤석열 \"공약 표절\" 논란에 인터뷰 명단 공개...\"동문서답\" 비판\n",
      "                                    \n"
     ]
    }
   ],
   "source": [
    "# html 가져오기\n",
    "url = \"https://news.naver.com/\"\n",
    "res = request.urlopen(url)\n",
    "\n",
    "# html 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "selector = \"#today_main_news > div.hdline_news > ul > li > div.hdline_article_tit > a\"\n",
    "\n",
    "for a in soup.select(selector):\n",
    "    title = a.text\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4e9a7",
   "metadata": {},
   "source": [
    "### 4.5 시민의 소리 게시판"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c59b2fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어린이대공원 매점 냉장고 점검 부탁드립니다.', '어린이를 위한 공원내 식당에 아기를 위한 시설 부족(아기의자가 왜 없죠?)', '강창수 해설사님 ', '동물해설사님 칭찬', '강창수 동물 해설사님', '놀이동산 푸드코트 김치가 중국산인 이유는?', '주슨트 설명 최고예요!!', '강창수 주슨트님 최고 !!', 'ZOOCENT 스케줄표?', '호주동물 호주설명 '] ['https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=nV7ZBaKGcSDuedYq32bQ98VFQYHSMeKxepwJhgye52w6bdM7pe6pdI3Rc1hQ10WD.etisw1_servlet_user?qnaid=QNAS20210925000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=nV7ZBaKGcSDuedYq32bQ98VFQYHSMeKxepwJhgye52w6bdM7pe6pdI3Rc1hQ10WD.etisw1_servlet_user?qnaid=QNAS20210923000005&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=nV7ZBaKGcSDuedYq32bQ98VFQYHSMeKxepwJhgye52w6bdM7pe6pdI3Rc1hQ10WD.etisw1_servlet_user?qnaid=QNAS20210920000001&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=nV7ZBaKGcSDuedYq32bQ98VFQYHSMeKxepwJhgye52w6bdM7pe6pdI3Rc1hQ10WD.etisw1_servlet_user?qnaid=QNAS20210919000004&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=nV7ZBaKGcSDuedYq32bQ98VFQYHSMeKxepwJhgye52w6bdM7pe6pdI3Rc1hQ10WD.etisw1_servlet_user?qnaid=QNAS20210919000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=nV7ZBaKGcSDuedYq32bQ98VFQYHSMeKxepwJhgye52w6bdM7pe6pdI3Rc1hQ10WD.etisw1_servlet_user?qnaid=QNAS20210918000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=nV7ZBaKGcSDuedYq32bQ98VFQYHSMeKxepwJhgye52w6bdM7pe6pdI3Rc1hQ10WD.etisw1_servlet_user?qnaid=QNAS20210909000001&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=nV7ZBaKGcSDuedYq32bQ98VFQYHSMeKxepwJhgye52w6bdM7pe6pdI3Rc1hQ10WD.etisw1_servlet_user?qnaid=QNAS20210908000004&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=nV7ZBaKGcSDuedYq32bQ98VFQYHSMeKxepwJhgye52w6bdM7pe6pdI3Rc1hQ10WD.etisw1_servlet_user?qnaid=QNAS20210906000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=nV7ZBaKGcSDuedYq32bQ98VFQYHSMeKxepwJhgye52w6bdM7pe6pdI3Rc1hQ10WD.etisw1_servlet_user?qnaid=QNAS20210904000006&pgno=1']\n"
     ]
    }
   ],
   "source": [
    "# html가져오기\n",
    "url_head = \"https://www.sisul.or.kr\"\n",
    "url_board = url_head + \"/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\"\n",
    "res = request.urlopen(url_board)\n",
    "\n",
    "# html 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출하기\n",
    "selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
    "titles = []\n",
    "links = []\n",
    "for a in soup.select(selector):\n",
    "    titles.append(a.text)\n",
    "    links.append(url_head + a.attrs[\"href\"])\n",
    "    \n",
    "print(titles, links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
